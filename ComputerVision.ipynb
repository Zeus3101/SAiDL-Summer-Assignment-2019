{"cells":[{"metadata":{"id":"20pfZV-DsU4B","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport cv2\n\nimport os\nimport time\nimport copy\n\nfrom PIL import Image\n\nimport torch\nfrom torch import optim, nn\nfrom torch.autograd import Variable\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader","execution_count":1,"outputs":[]},{"metadata":{"id":"Sfw7W9vosU4E","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def create_pathlist(root='/content/drive/My Drive/data'):\n    pathlist = {'train' : [],\n                'val' : [],\n                'test' : []}\n\n    for r, d, f in os.walk(root):\n        for path in f:\n            if '.avi' not in path:\n                continue\n            group = int(path.split('_')[-2])\n            full_path = os.path.join(r, path)\n\n            if group <= 20:\n                split = 'train'\n            elif group <= 22:\n                split = 'val'\n            else:\n                split = 'test'\n\n            pathlist[split].append(full_path)\n    \n    return pathlist","execution_count":2,"outputs":[]},{"metadata":{"id":"g2djblhAsU4G","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"pathlist = create_pathlist()","execution_count":3,"outputs":[]},{"metadata":{"id":"HQQyoNWIsU4I","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"transform = {'train' : transforms.Compose([transforms.RandomResizedCrop(196, scale=[0.8, 1.0]),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.RandomRotation(24),\n                                           transforms.CenterCrop(144),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n             \n             'val' : transforms.Compose([transforms.RandomResizedCrop(196, scale=[0.8, 1.0]),\n                                         transforms.CenterCrop(144),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n             \n             'test' : transforms.Compose([transforms.RandomResizedCrop(196, scale=[0.8, 1.0]),\n                                          transforms.CenterCrop(144),\n                                          transforms.ToTensor(),\n                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}","execution_count":4,"outputs":[]},{"metadata":{"id":"8gsKfSFfsU4K","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"class VideoFolder(Dataset):\n    def __init__(self, root, split, transform=None):\n        self.pathlist = create_pathlist(root)[split]\n        self.split = split\n        self.transform = transform\n        self.batch_size = 16\n        self.num = 0\n        self.classes = {'shooting' : 0,\n                        'biking' : 1,\n                        'diving' : 2,\n                        'golf' : 3,\n                        'riding' : 4,\n                        'juggle' : 5,\n                        'swing' : 6,\n                        'tennis' : 7,\n                        'jumping' : 8,\n                        'spiking' : 9,\n                        'walk' : 10}\n        \n    \n    def __len__(self):\n        return len(self.pathlist)\n    \n\n    def video_read(self, j):\n        path = self.pathlist[j]\n\n        label = self.classes[path.split('/')[-1].split('_')[1]]\n\n        capture = cv2.VideoCapture(path)\n\n        num_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        frame_list = np.random.choice(num_frames, self.batch_size, replace=False)\n\n        image_frames = []\n        labels = [label for _ in range(self.batch_size)]\n\n        for i in range(num_frames):\n            if i not in frame_list:\n                continue\n\n            running, frame = capture.read()\n            if not running:\n                break\n                \n            image = Image.fromarray(frame)\n            image = self.transform(image)\n\n            image_frames.append(image)\n\n        return image_frames, label\n    \n    \n    def __getitem__(self, index):\n        image_frames, label = self.video_read(index)\n        \n        images = torch.stack([frame for frame in image_frames])\n        labels = torch.stack([torch.LongTensor([label for _ in range(self.batch_size)])]).reshape((-1))\n        \n        return (images, labels)\n    \n    \n    def __iter__(self):\n        return self\n    \n    \n    def __next__(self):\n        try:\n            num = self.num\n            self.num += 1\n            return self[num]\n        except:\n            self.num = 0\n            raise StopIteration","execution_count":5,"outputs":[]},{"metadata":{"id":"_7dnFgqSsU4P","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"datasets = {x : VideoFolder('/content/drive/My Drive/data', x, transform[x]) for x in ['train', 'val', 'test']}","execution_count":6,"outputs":[]},{"metadata":{"id":"qGaoIgB0zRma","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"gpu = torch.cuda.is_available()","execution_count":7,"outputs":[]},{"metadata":{"id":"QXs2MerRsU4U","colab_type":"code","outputId":"e5e054e3-c5a6-4079-c285-3baa4ddff224","colab":{"base_uri":"https://localhost:8080/","height":773},"trusted":true},"cell_type":"code","source":"model = models.vgg16(pretrained=True)\n\ntorch.cuda.empty_cache()\n\nmodel.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 11)\n        )\n\nmodel.cuda()","execution_count":8,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /tmp/.cache/torch/checkpoints/vgg16-397923af.pth\n100%|██████████| 553433881/553433881 [00:04<00:00, 111792245.14it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace)\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace)\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=11, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{"id":"O-Ly0R36sU4W","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.classifier.parameters(), lr=0.01)","execution_count":9,"outputs":[]},{"metadata":{"id":"yYyCwD-UH2--","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def train(model, optimizer, criterion, num_epochs=10, lr=0.01):\n    start = time.time()\n    best_model_state = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        for images, labels in datasets['train']:\n            images = Variable(images.cuda())\n            labels = Variable(labels.cuda())\n\n            optimizer.zero_grad()\n\n            outputs = model(images)\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            optimizer.step()\n        print(\"Train:               Epoch: {}, Loss: {}\".format(epoch+1, loss.item()))\n\n        correct = 0\n\n        for images, labels in datasets['val']:\n            images = Variable(images.cuda())\n            labels = Variable(labels.cuda())\n\n            with torch.set_grad_enabled(False):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            _, predicted = torch.max(outputs.data, 1) \n            correct += torch.sum(predicted.cpu() == labels.cpu()).item()\n        \n        accuracy = correct / len(datasets['val'])\n        accuracy /= 16\n        print(\"Validation:          Epoch: {}, Loss: {}, Accuracy: {}\".format(epoch+1, str(loss.item()), str(accuracy)))\n\n        if accuracy > best_acc:\n            best_model_state = copy.deepcopy(model.state_dict)\n            best_acc = accuracy\n\n    stop = time.time()\n\n    print(\"Time taken: {:.4f}\".format(stop-start))\n    print(\"Best accuracy: {:.4f} %\".format(100 * best_acc))\n    \n    model = model.load_state_dict(best_model_state)\n    return model","execution_count":10,"outputs":[]},{"metadata":{"id":"1Yx2SSql6IcJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"model = train(model, optimizer, criterion)","execution_count":11,"outputs":[{"output_type":"stream","text":"Train:               Epoch: 1, Loss: 0.001557469367980957\nValidation:          Epoch: 1, Loss: 11.155577659606934, Accuracy: 0.10687022900763359\nTrain:               Epoch: 2, Loss: 0.0005014538764953613\nValidation:          Epoch: 2, Loss: 10.972640991210938, Accuracy: 0.10687022900763359\nTrain:               Epoch: 3, Loss: 5.97834587097168e-05\nValidation:          Epoch: 3, Loss: 12.392721176147461, Accuracy: 0.10687022900763359\nTrain:               Epoch: 4, Loss: 0.00010114908218383789\nValidation:          Epoch: 4, Loss: 12.91894245147705, Accuracy: 0.10782442748091603\nTrain:               Epoch: 5, Loss: 1.7523765563964844e-05\nValidation:          Epoch: 5, Loss: 13.316166877746582, Accuracy: 0.11211832061068702\nTrain:               Epoch: 6, Loss: 3.8743019104003906e-05\nValidation:          Epoch: 6, Loss: 12.719279289245605, Accuracy: 0.125\nTrain:               Epoch: 7, Loss: 0.0\nValidation:          Epoch: 7, Loss: 19.84551429748535, Accuracy: 0.10687022900763359\nTrain:               Epoch: 8, Loss: 1.0073184967041016e-05\nValidation:          Epoch: 8, Loss: 17.289596557617188, Accuracy: 0.11498091603053436\nTrain:               Epoch: 9, Loss: 1.6927719116210938e-05\nValidation:          Epoch: 9, Loss: 16.193410873413086, Accuracy: 0.13263358778625955\nTrain:               Epoch: 10, Loss: 2.4318695068359375e-05\nValidation:          Epoch: 10, Loss: 15.262426376342773, Accuracy: 0.14694656488549618\nTime taken: 212.7472\nBest accuracy: 14.6947 %\n","name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"'function' object has no attribute 'copy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-13294cfe0099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-7eb663ec2890>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, num_epochs, lr)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best accuracy: {:.4f} %\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;31m# copy state_dict so _load_from_state_dict can modify it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_metadata'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'copy'"]}]},{"metadata":{"id":"51crgHanEJiq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"feature_extractor_3.ipynb","version":"0.3.2","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":1}