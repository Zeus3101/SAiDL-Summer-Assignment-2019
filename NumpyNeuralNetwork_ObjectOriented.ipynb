{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This is a notebook to make a working Numpy Neural Network and discuss its working. All variable names will be in bold when referred to in markdown cells.\n",
    "\n",
    "## 1.1. Setup\n",
    "\n",
    "First thing we need to do is create the data set that we need. For that, we need to decide how we take the input and how we give the output. In this case, I have decided to take input as a string with 5 numbers separated by spaces. The first two and will be the first binary number we need and the third and fourth digits will be the second binary number. The last number will decide whether we want an XOR or an XNOR. The output will be an array of two numbers which will be our binary result.\n",
    "\n",
    "I'm going to use numpy arrays to represent inputs and outputs. The input will be a numpy array of shape 5 and the output/label array will have a size of 2.\n",
    "\n",
    "For this, we'll need to make a function to compute the XOR and another to compute the XNOR. Once we do that, we generate the required dataset with a function which will output our training set __train__ and their respective __labels__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We send an input of a numpy array sized 4 which contains the input binary numbers.\n",
    "\n",
    "def xor(arr):\n",
    "    res = [0, 0]\n",
    "    \n",
    "    if arr[0] != arr[2]:\n",
    "        res[0] = 1\n",
    "    \n",
    "    if arr[1] != arr[3]:\n",
    "        res[1] = 1\n",
    "    \n",
    "    return res\n",
    "\n",
    "def xnor(arr):\n",
    "    res = [0, 0]\n",
    "    \n",
    "    if arr[0] == arr[2]:\n",
    "        res[0] = 1\n",
    "    \n",
    "    if arr[1] == arr[3]:\n",
    "        res[1] = 1\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we create a function to map all possible values of the inputs to all their respective labels \n",
    "and create two numpy arrays shaped (32, 5) and (32, 2) respectively called train and labels.\n",
    "\"\"\"\n",
    "\n",
    "def createTrain():\n",
    "    train = np.zeros((32, 5))\n",
    "    labels = np.zeros((32, 2))\n",
    "    \n",
    "    temp = np.zeros((1, 7))\n",
    "    count = 0\n",
    "    \n",
    "    for a1 in range(2):\n",
    "        for a2 in range(2):\n",
    "            for a3 in range(2):\n",
    "                for a4 in range(2):\n",
    "                    for choice in range(2):\n",
    "                        if choice is 0:\n",
    "                            temp = np.array([a1, a2, a3, a4] + [choice])\n",
    "                            train[count] = temp\n",
    "                            labels[count] = xor([a1, a2, a3, a4])\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            temp = np.array([a1, a2, a3, a4] + [choice])\n",
    "                            train[count] = temp\n",
    "                            labels[count] = xnor([a1, a2, a3, a4])\n",
    "                            count += 1\n",
    "    \n",
    "    return (train.astype(int), labels.astype(int))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, labels = createTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Structure of Neural Network\n",
    "\n",
    "Now, we need to decide how our Neural Network is going to look. In this case, I'm going to take a simple feed forward neural network with 2 hidden layers. Our input layer will have a dimension of 5 and both the hidden layers will have dimensions of 8 each and the output layer will have dimensions 2. The activation functions for all of them will be ReLU except for the last layer which will have a sigmoid so that our final values will be between 0 and 1.\n",
    "\n",
    "For clarity, we'll declare a variable called __nn_structure__ to define these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_structure = [{\"input_dim\" : 5, \"output_dim\" : 8, \"activation\" : 'sigmoid'},\n",
    "                {\"input_dim\" : 8, \"output_dim\" : 8, \"activation\" : 'sigmoid'},\n",
    "                {\"input_dim\" : 8, \"output_dim\" : 2, \"activation\" : 'sigmoid'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Activation Functions\n",
    "\n",
    "Let's define the activation functions and their derivatives which will be needed for backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-78cb1e686247>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-78cb1e686247>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    def softmax(Z)\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / float(sum(np.exp(Z)))\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dA[Z < 0] = 0\n",
    "    return dA\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    return dA * sigmoid(Z) * (1 - sigmoid(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup Neural Network\n",
    "\n",
    "Now that we have setup our datasets and know what our neural network is going to look like, let's create a model class. So our model class needs to train on the given dataset and then be able to take input and then predict the output. \n",
    "\n",
    "For this we'll need the following functions:\n",
    "1. \\_\\_init\\_\\_ : This will store the structure required and create some useful variables\n",
    "2. init_layers : This will initialise weights and biases randomly\n",
    "3. forward : This will only go through one layer\n",
    "4. full_forward : This will apply forward on all the layers\n",
    "5. loss : This will calculate the cross-entropy loss for each entry\n",
    "6. backward : This will go back one layer and store what needs to be updated\n",
    "7. full_backward : This will apply backward on all layers\n",
    "8. update : This will update all our parameters by Gradient Descent\n",
    "9. get_accuracy : This will calculate accuracy of our predictions by comparing with the respective labels\n",
    "\n",
    "Our weights, biases and calculated neuron values of the next layer will be represented by _W , b and Z_ followed by the index of the layer (starting from layer 1) they are in. The actual values of the layers (activated neuron values) will be represented by A followed by the index of the layer (starting from layer 0). The final value will be represented by __A__ which will be our output. _A0_ will represent the actual neuron values of the first layer (the input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyNeuralNetwork():\n",
    "    def __init__(self, nn_structure):\n",
    "        self.structure = nn_structure\n",
    "        self.parameters = {}\n",
    "        self.memory = {}\n",
    "        self.update = {}\n",
    "        self.cost_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "        for i, layer in enumerate(self.structure):\n",
    "            layer_index = i + 1\n",
    "\n",
    "            input_dim = layer[\"input_dim\"]\n",
    "            output_dim = layer[\"output_dim\"]\n",
    "\n",
    "            self.parameters['W' + str(layer_index)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            self.parameters['b' + str(layer_index)] = np.random.randn(output_dim, 1) * 0.1\n",
    "    \n",
    "    # A_ represents the neuron values of the current layer. A represents the neuron values of the next layer\n",
    "    def forward(self, A_, W, b, activation):\n",
    "        Z = np.dot(W, A_) + b\n",
    "\n",
    "        if activation is \"relu\":\n",
    "            act = relu\n",
    "        elif activation is \"sigmoid\":\n",
    "            act = sigmoid\n",
    "        else:\n",
    "            raise Exception(\"Activation not found\")\n",
    "        \n",
    "        A = act(Z)\n",
    "        \n",
    "        return (A, Z)\n",
    "    \n",
    "    # In the following function, X is our input in the form of a numpy array of size 5\n",
    "    def full_forward(self, X):\n",
    "        A = X\n",
    "\n",
    "        for i, layer in enumerate(self.structure):\n",
    "            layer_index = i + 1\n",
    "\n",
    "            A_ = A\n",
    "\n",
    "            act = layer[\"activation\"]\n",
    "\n",
    "            W = self.parameters['W' + str(layer_index)]\n",
    "            b = self.parameters['b' + str(layer_index)]\n",
    "\n",
    "            A, Z = self.forward(A_, W, b, act)\n",
    "            \n",
    "            self.memory['A' + str(i)] = A_\n",
    "            self.memory['Z' + str(layer_index)] = Z\n",
    "\n",
    "        return A\n",
    "    \n",
    "    # Next, we'll calculate the cross entropy loss for our parameters in the next function\n",
    "    def loss(self, A, Y):\n",
    "        cost = np.sum(np.dot(Y, np.log(A).T) + np.dot(1-Y, np.log(1-A).T))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def MSEloss(self, A, Y):\n",
    "        cost = 1 / 2 * np.sum((A - Y)**2)\n",
    "        return np.squeeze(cost)\n",
    "\n",
    "    # Next, we'll calculate the backward by differentiating the cross entropy loss\n",
    "    def backward(self, dA, W, b, Z, A_, activation):\n",
    "        if activation is \"relu\":\n",
    "            act = relu_backward\n",
    "        elif activation is \"sigmoid\":\n",
    "            act = sigmoid_backward\n",
    "        else:\n",
    "            raise Exception(\"Activation not found\")\n",
    "\n",
    "        dZ = act(dA, Z)\n",
    "        dW = np.dot(dZ, A_.T)\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_ = np.dot(W.T, dZ)\n",
    "\n",
    "        return dA_, dW, db\n",
    "    \n",
    "    # Same as the full_forward function, we make a corresponding full_backward function\n",
    "    def full_backward(self, A, labels):\n",
    "        labels = labels.reshape(A.shape)\n",
    "\n",
    "        #dA_ = - (A - labels)\n",
    "        dA_ = - (np.divide(labels, A) - np.divide(1 - labels, 1 - A))\n",
    "\n",
    "        for i_, layer in reversed(list(enumerate(self.structure))):\n",
    "            i = i_ + 1\n",
    "            act = layer[\"activation\"]\n",
    "\n",
    "            dA = dA_\n",
    "\n",
    "            A_ = self.memory[\"A\" + str(i_)]\n",
    "            Z = self.memory[\"Z\" + str(i)]\n",
    "\n",
    "            W = self.parameters[\"W\" + str(i)]\n",
    "            b = self.parameters[\"b\" + str(i)]\n",
    "\n",
    "            dA_, dW, db = self.backward(dA, W, b, Z, A_, act)\n",
    "\n",
    "            self.update[\"dW\" + str(i)] = dW\n",
    "            self.update[\"db\" + str(i)] = db\n",
    "    \n",
    "    # Now that we have calculated the backward for all the parameters, we update them\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for i, layer in enumerate(self.structure):\n",
    "            layer_index = i + 1\n",
    "\n",
    "            self.parameters['W' + str(layer_index)] -= learning_rate * self.update['dW' + str(layer_index)]\n",
    "            self.parameters['b' + str(layer_index)] -= learning_rate * self.update['db' + str(layer_index)]\n",
    "     \n",
    "    # We could use a function to calculate the accuracy of the model too\n",
    "    def get_accuracy(self, A, labels, x=0.5):\n",
    "        A_ = np.array(A, copy=True)\n",
    "        A_[A_ < x] = 0\n",
    "        A_[A_ >= x] = 1\n",
    "\n",
    "        return (A_ == labels).mean()\n",
    "    \n",
    "    # Let's make a function to train the model\n",
    "    def train(self, train, labels, num_epochs, lr=0.1):\n",
    "        self.cost_history, self.accuracy_history = [], []\n",
    "        for _ in range(num_epochs):\n",
    "            for i in range(len(train)):\n",
    "                X = train[i].reshape((-1, 1))\n",
    "                Y = labels[i].reshape((-1, 1))\n",
    "\n",
    "                A = self.full_forward(X)\n",
    "\n",
    "                self.full_backward(A, Y)\n",
    "                \n",
    "                self.update_parameters(lr)\n",
    "\n",
    "                accuracy = self.get_accuracy(A, Y, 0.5)\n",
    "                self.accuracy_history.append(accuracy)\n",
    "\n",
    "            cost = self.MSEloss(A, Y)\n",
    "            print(cost)\n",
    "            self.cost_history.append(cost)\n",
    "    \n",
    "    # Lastly, let's make a function so that we can pass any random input\n",
    "    def __call__(self, X):\n",
    "        X = np.array([int(x) for x in X.split(' ')]).reshape((-1, 1))\n",
    "        \n",
    "        A = X\n",
    "        \n",
    "        for i, layer in enumerate(self.structure):\n",
    "            layer_index = i + 1\n",
    "\n",
    "            A_ = A\n",
    "\n",
    "            act = layer[\"activation\"]\n",
    "\n",
    "            W = self.parameters['W' + str(layer_index)]\n",
    "            b = self.parameters['b' + str(layer_index)]\n",
    "\n",
    "            A, Z = self.forward(A_, W, b, act)\n",
    "        \n",
    "        print(A)\n",
    "        A[A > 0.5] = 1\n",
    "        A[A <= 0.5] = 0\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumpyNeuralNetwork(nn_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33144915212718484\n",
      "0.3239250221836148\n",
      "0.3174885989523412\n"
     ]
    }
   ],
   "source": [
    "model.train(train, labels, 3, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.array(model.accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[acc==0.5] = 0\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
